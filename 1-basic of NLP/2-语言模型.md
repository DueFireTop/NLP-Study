# 2. 语言模型

## 2.1 统计语言模型简介

### 2.1.1 语言模型任务

> **原始语句**：南京市长江大桥
>
> **Sentence_1**：南京；市长；江大桥
>
> **Sentence_2**：南京市；长江大桥
>
> 很明显，第二种更合理。

**模型**指的是对事物的数学抽象，那么**语言模型**指的就是对语言现象的数学抽象

**定义**：语言模型任务是给句子一个**分配概率**的任务，也就是计算 $P(w)$，$w$ 是一句话。此外，语言模型也对**给定单词**在多个单词也就是一个**词序列**之后的可能性进行分析，并分配概率。

**语料库**：采样世界语句集合，得到的子集，就是样本空间。

**贝叶斯公式**：$P(AB) = P(A|B)P(B)=P(B|A)P(A)$



定义的数学解释：分配一个词在序列之后的概率（条件概率），给任意词序列分配概率。

语言模型就是**给任何词序列分配一个概率**。
$$
P(w_i|w_1,w_2,...,w_{i-1})=\frac{P(w_1,w_2,...,w_{i-1},w_i)}{P(w_1,w_2,...,w_{i-1})}\tag{2.1.1}
$$
我们定义一个**统计模型**如下：
$$
P(w_{1;n})=P(w_1)P(w_2|w_1)P(w_3|w_{1:2})P(w_4|w_{1:3})...P(w_n|w_{1:n-1})\tag{2.1.2}
$$

**总结**：语言模型任务的完美表现是预测序列中的下一个单词具有与人类参与者所需的相同或更低的猜测数目，这是人类智能的体现。此外，语言模型还可以用于对机器翻译和语音识别的结果进行**打分**。语言模型是NLP、人工智能、机器学习研究的主要角色。

**缺点**：

1. 数据稀疏，越长的句子概率越低。
2. 计算代价大，句子越长，要根据句子来索引计算和储存的 p 就越多。
3. 最后一个词取决于这句话前面的所有词，是的高效建模整句话的难度大。



### 2.1.2 改进方法

最经典和传统的改进方法，就是**n阶马尔可夫模型**，也叫**n元语法**或者**N-Gram**。

一个k阶马尔可夫假设，假设序列中下一个词的概率**只依赖于其前k个词**。用公式表示如下（ $i $ 为要预测的词的位置或索引，$w$ 代表词）：
$$
P(w_{i+1}|w_{1,i}) \approx P(w_{i+1}|w_{i-k_ii})\tag{2.1.3}
$$
那句子的概率估计就变为：
$$
P(w_{1:n}) \approx \prod_{i=1}^{n} P(w_i|w_{i-k:i-1})\tag{2.1.4}
$$



### 2.1.3 案例实现

#### 【1】语料预处理

```py
from collections import Counter
import numpy as np


"""语料"""
corpus = '''她的菜很好 她的菜很香 她的他很好 他的菜很香 他的她很好
很香的菜 很好的她 很菜的他 她的好 菜的香 他的菜 她很好 他很菜 菜很好'''.split() # 以空格为分隔符，将语料分开


"""语料预处理"""
counter = Counter()  # 词频统计
for sentence in corpus:
    for word in sentence:
        counter[word] += 1
counter = counter.most_common()
lec = len(counter)
word2id = {counter[i][0]: i for i in range(lec)}
id2word = {i: w for w, i in word2id.items()}
print(word2id)
print(id2word)
```

![image-20230613163255350](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306131633949.png)

#### 【2】N-Gram建模训练

```py
"""N-gram建模训练"""
unigram = np.array([i[1] for i in counter]) / sum(i[1] for i in counter)
print(unigram)

print("-------------------字到序号的转变-------------------")
bigram = np.zeros((lec, lec))   # + 1e-8
for sentence in corpus:
    sentence = [word2id[w] for w in sentence]
    print(sentence)
    for i in range(1, len(sentence)):
        bigram[[sentence[i - 1]], [sentence[i]]] += 1  # 对应词表位置词频加一（第一个词是3并且第二个词是0的情况词频加一）

print("-------------------相邻两个词出现的数量-------------------")
print(bigram)

print("-------------------相邻两个词出现的数量归一化-------------------")
for i in range(lec):
    bigram[i] /= bigram[i].sum()  # 对词频归一化，变为概率
print(bigram)

'''
  [0.2	0.2	0.16363636	0.12727273	0.12727273	0.10909091	0.07272727]
-------------------字到序号的转变-------------------
[3, 0, 2, 1, 4]
[3, 0, 2, 1, 6]
[3, 0, 5, 1, 4]
[5, 0, 2, 1, 6]
[5, 0, 3, 1, 4]
[1, 6, 0, 2]
[1, 4, 0, 3]
[1, 2, 0, 5]
[3, 0, 4]
[2, 0, 6]
[5, 0, 2]
[3, 1, 4]
[5, 1, 2]
[2, 1, 4]
-------------------相邻两个词出现的数量-------------------
[[0. 0. 5. 2. 1. 2. 1.]
 [0. 0. 2. 0. 6. 0. 3.]
 [2. 4. 0. 0. 0. 0. 0.]
 [4. 2. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0.]
 [3. 2. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0.]]
-------------------相邻两个词出现的数量归一化-------------------
[[0.         0.         0.45454545 0.18181818 0.09090909 0.18181818	0.09090909]
 [0.         0.         0.18181818 0.         0.54545455 0.					0.27272727]
 [0.33333333 0.66666667 0.         0.         0.         0.					0.        ]
 [0.66666667 0.33333333 0.         0.         0.         0.					0.        ]
 [1.         0.         0.         0.         0.         0.					0.        ]
 [0.6        0.4        0.         0.         0.         0.					0.        ]
 [1.         0.         0.         0.         0.         0.					0.        ]]
'''
```

![image-20230613164944138](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306131649183.png)

> **注意**：
>
> ![image-20230613164514306](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306131645346.png)

#### 【3】句子概率

$P(w_{1:n}) \approx \prod_{i=1}^{n} P(w_i|w_{i-k:i-1})$

```py
"""句子概率"""
def prob(sentence):
    s = [word2id[w] for w in sentence]
    les = len(s)
    if les < 1:
        return 0
    p = unigram[s[0]]
    if les < 2:
        return p
    for i in range(1, les):
        p *= bigram[s[i - 1], s[i]] #根据上边的概率矩阵计算得到
    return p

print('很好的菜', prob('很好的菜'))
print('菜很好的', prob('菜很好的'))
print('菜好的很', prob('菜好的很'))
```

![image-20230613165304020](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306131653056.png)

是根据上边的概率矩阵计算得到的，$P(w_1,w_2,w_3,w_4) = P(w_1)P(w_2|w_1)P(w_3|w_2)P(w_4|w_3)$ ，是基于2-Gram计算的。

#### 【4】生成排列组合

```py
"""排列组合"""
def permutation_and_combination(ls_ori, ls_all=None):
    ls_all = ls_all or [[]]
    le = len(ls_ori)
    if le == 1:
        ls_all[-1].append(ls_ori[0])
        ls_all.append(ls_all[-1][: -2])
        return ls_all
    for i in range(le):
        ls, lsi = ls_ori[:i] + ls_ori[i + 1:], ls_ori[i]
        ls_all[-1].append(lsi)
        ls_all = permutation_and_combination(ls, ls_all)
    if ls_all[-1]:
        ls_all[-1].pop()
    else:
        ls_all.pop()
    return ls_all

print('123排列组合', permutation_and_combination([1, 2, 3]))


"""给定词组，返回最大概率组合的句子"""
def max_prob(words):
    pc = permutation_and_combination(words)  # 生成排列组合
    p, w = max((prob(s), s) for s in pc)
    return p, ''.join(w)

print(*max_prob(list('香很的菜')))
print(*max_prob(list('她的菜香很')))
print(*max_prob(list('好很的他菜')))
print(*max_prob(list('好很的的她菜')))
```

![image-20230613170108129](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306131701166.png)



## 2.2 语言模型任务评估

### 2.2.1 困惑度：PPL

在实践中，我们不使用原始概率作为我们评估语言模型的度量标准，而是使用一种叫做**困惑度（perplexity, PPL）**的变量。测试集上的语言模型的PPL是测试集上的【[逆反概率](# 逆反概率)】，由单词的数量归一化。对于一个测试集 $W=w_1w_2...w_N$：


$$
\begin{align}
PPL(W)&=P(w_1w_2...w_N)^{-\frac{1}{N}}\\
&=\sqrt[N]{\frac{1}{P(w_1w_2...w_N)}}\\
&=\sqrt[N]{\prod_{i=1}^{N}\frac{1}{P(w_i|w_{1:i-1})} }
\end{align} \tag{2.2.1}
$$
测试集 $W$ 的困惑度取决于我们使用哪种语言模型。以下是我们用一个一元语言模型（unigram）来计算 $W$ 的困惑度：
$$
PPL(W)=\sqrt[N]{\prod_{i=1}^{N}\frac{1}{P(w_i)}}\tag{2.2.2}
$$
用二元语言模型（bigram）计算的W的困惑仍然是几何平均值，但现在是二元概率：
$$
PPL(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i|w_{i-1})}}\tag{2.2.3}
$$

注意，由于等式（2.2.1）中的倒数，如果单词序列的条件概率越高，则困惑度越低。因此，最小化困惑度就等同于根据语言模型最大化测试集的概率。我们通常在等式（2.2.1）或等式（2.2.3）中用到的单词序列的是某个测试集中的整个单词序列。



还有另外一种思考PPL的方式：作为一种语言的**加权平均分支因子**。在一种语言中，`分支因子`是指【任何单词】后面【可能出现的下一个单词】的数量。假设一个识别英文数字 $(zero, one, two,..., nine)$ 的任务，假定这10个数字中的每个数字都以相同的概率 $P = \frac{1}{10}$ 出现，那么这个小型语言的困惑度实际上是10。推导一下：
$$
\begin{align}
PPL(W)&=P(w_1w_2...w_N)^{-\frac{1}{N}}\\
&=(\frac{1}{10})^{N\times (-\frac{1}{N})}\\
&=10
\end{align}\tag{2.2.4}
$$


假设数字 $zero$ 的出现频率比其他数字高很多，则这个测试集的困惑度会很低，因为大多数时候下一个数字是 $zero$ ，这是很容易预测的，也就是说有很高的概率。因此，虽然分支因子仍为10，但是困惑度或加权分支因子比较小。



下图显示了根据每种语法的150万个单词WSJ测试集的困惑度：

![image-20230614110147848](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306141102221.png)

N-Gram提供给我们有关单词序列的信息越多，困惑度就越低。需要注意的是，在计算困惑度时，N-Gram模型 $P$ 必须在不了解测试集的情况下构造，也不需要实现了解测试集的词汇量。测试集的任何知识都可能导致困惑度被人为降低。两种语言模型的困惑度也只有在使用相同词汇时才具有可比性。

困惑度（内在）的改善并不能保证语言处理任务性能（外在）的改善。尽管如此，困惑度往往与这些改进相关，它通常被用作对算法的快速检查。但是，在最后对模型进行评价之前，模型在困惑方面的改进应该总是通过对真实任务的端到端评估来确认。



> **困惑度**：困惑度是一种信息论测度，用来测量一个概率模型预测样本的好坏，困惑度越低越好。给定一个包含n个词的文本预料（n可以数以百万计）和一个基于词语历史的用于为词语分配概率的语言模型函数LM，LM在一句话上的困惑度是$2^{-\frac{1}{n}\sum_{i=1}^n\log_2{LM(w_i|w_{1:i-1})}}$。



### 2.2.2 困惑度与熵的关系

更好的N-Gram模型时为测试数据分配更高概率的模型，而困惑度是测试集概率的归一化版本。困惑度实际上源于**交叉熵**的信息理论概念，他解释了困惑度的神秘性质及其与熵的关系。

#### 【1】熵

数学是一种工具，使用数学来描述现实中的各种事物是一个数学家本质的工作目标，而现实中不确定，或者说不太确定是否会发生的事件必须要找到一种抽象的、符号化和公式化的手段去表示。

**熵（entropy）**是信息的度量。给定一个随机变量 $X$ ，它的范围在我们所预测的任何事物之上（单词、字母、词性，我们称之为 $\mathcal{X}$ ），并具有特定的概率函数，将其称为 $p(x)$ ，随机变量 $X$ 的熵是：
$$
H(X)=-\sum_{x\in \mathcal{X}}p(x)\log_2{p(x)}\tag{2.2.5}
$$
原则上，对数 $\log$ 可以在任何底数 $base$ 下计算，如果我们使用以 2 为底数的对数，则得到的熵值将是以比特来度量的。一种直观的理解熵的方式是：在最优编码方案中，熵是编码某个决策或信息所需要的比特数的下界。

信息量不等于信息熵，如果是这样的话，直接用概率来衡量就可以了，不需要再重新定义一个概念。信息熵就是描述**整个概率分布的不确定性**。

> **示例**：
>
> 假设我们对天气的概率一无所知，那么这四种天气发生的概率为等概率（服从平均分布）。$P = [p_1,p_2,p_3,p_4] = [\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}]$。代入信息熵公式，计算可得：$H(P)=2$。
>
> 继续思考，假设我们考虑天气的城市是一个地处中国南方雨季的城市，那么阴天和雨天的概率从经验角度（先验概率）来看大于晴天、雪天，我们把这种分布记为$Q=[\frac{1}{4},\frac{1}{8},\frac{1}{2},\frac{1}{8}]$。代入信息熵公式，计算可得：$H(Q)=1.75$。
>
> 观察一下信息熵值的变化，和我们之前建立的直观理解完全符合。
>
> |                 天气【阴，晴，雨，雪】                  |   信息熵    |
> | :-----------------------------------------------------: | :---------: |
> |  $P=[\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}]$  |  $H(P)=2$   |
> |  $Q=[\frac{1}{4},\frac{1}{8},\frac{1}{2},\frac{1}{8}]$  | $H(Q)=1.75$ |
> | $Z=[\frac{1}{8},\frac{1}{16},\frac{3}{4},\frac{1}{16}]$ | $H(Z)=1.29$ |
> |                      $W=[0,0,1,0]$                      |  $H(W)=0$   |



到目前为止，我们一直在计算单个变量的熵。但是我们用熵来做的大部分事情都涉及到序列。例如，对于语法，我们将计算单词 $W=\left \{ w_0, w_1,w_2,...,w_n \right \} $ 序列的熵。一种实现方法是使用一个表示单词序列的变量。例如，我们可以计算一个随机变量（某种语言 $L$ 中长度为 n 的所有有限单词序列）的熵，如下所示：
$$
H(w_1,w_2,...,w_n)=-\sum_{w_{1:n}\in L}p(w_{1:n})\log{p(w_{1:n})}\tag{2.2.6}
$$
我们可以定义**熵率 entropy rate（我们也可以把它看作是每个词的熵）**为该序列的熵除以单词数：
$$
\frac{1}{n}H(w_{1:n})=-\frac{1}{n}\sum_{w_{1:n}\in L}p(w_{1:n})\log{p(w_{1:n})}\tag{2.2.7}
$$
但是为了测量一种语言的真实熵，我们需要考虑无限长度的序列。如果我们把语言看作一个【产生一系列单词的】随机过程 $L$，并允许 $W$ 代表单词序列 $w_1,w_2, ..., w_n$ ，则 $L$ 的熵率 $H(L)$ 定义为：
$$
\begin{align}
H(L) &=\lim_{n\to \infty}{\frac{1}{n}H(w_1.w_2,...,w_n)}\\
&=-\lim_{n \to \infty}{\frac{1}{n}\sum_{W\in L}{p(w_1,w_2,...,w_n)\log{p(w_1,w_2,...,w_n)}}}
\end{align}\tag{2.2.8}
$$

Shannon-McMillan-Breiman定理指出，如果语言在某些方面是正则的（确切地说，如果它既是静止的又是遍历的），那么：
$$
H(L)=\lim_{n\to \infty}{-\frac{1}{n}\log{p(w_1,w_2,...,w_n)}}\tag{2.2.9}
$$
也就是说，我们可以取一个足够长的序列，而不是对所有可能的序列求和。Shannon-McMillan-Breiman定理的直觉是，一个足够长的单词序列将包含许多其他较短的序列，并且这些较短的序列中的每一个都将根据它们的概率在较长的序列中重复出现。

如果一个随机过程分配给一个序列的概率相对于时间指数的变化是不变的，那么这个随机过程就被称为**平稳**的。换句话说，单词在时间t的概率分布与时间t+1的概率分布相同。马尔可夫模型和 N-Grams 都是平稳的。但是，自然语言并不是平稳的，即将出现的单词的概率可能取决于任意距离和时间相关的时间，因此，我们的统计模型仅给出自然语言的正确分布和熵的近似值。

总之，通过做一些不正确但是很方便的简化假设，我们可以计算随机过程的熵（通过取一个很长的输出样本，并计算其平均对数概率）。

#### 【2】交叉熵

当我们不知道产生某些数据的实际概率分布 $p$ 时，**交叉熵**很有用。它允许我们使用一些 $m$，它是 $p$ 的模型（即 $p$ 的近似值）。$m$ 在 $p$ 上的交叉熵如下：
$$
H(p,m) = \lim_{n\to \infty}{-\frac{1}{n}\sum_{W\in L} p(w_1,w_2,...,w_n)\log{m(w_1,w_2,...,w_n)}}\tag{2.2.10}
$$
也就是说，我们根据概率分布 $p$ 绘制序列，但根据 $m$ 对其概率的对数求和。

再次，我们根据 Shannon-McMillan-Breiman 定理，进行平稳的遍历过程：
$$
H(p,m)=\lim_{n\to \infty}{-\frac{1}{n}\log{m(w_1,w_2,...,w_n)}}\tag{2.2.11}
$$
这意味着，对于熵，我们可以通过取一个足够长的序列来估计模型 $m$ 在某个分布 $p$ 上的交叉熵，而不是对所有可能的序列求和。使交叉熵之所以有用，是因为交叉熵 $H(p,m)$ 是熵 $H(p)$ 的上界。对于任何模型 $m$：
$$
H(p) \le H(p,m)\tag{2.2.12}
$$

这意味着我们可以使用一些简化的模型 $m$ 来帮助估计（根据概率 $p$ 来描述的符号序列的）真实熵。$m$ 越精确，交叉熵 $H(p,m)$ 就越接近真实熵 $H(p)$。因此，$H(p,m)$ 和 $H(p)$ 之间的差异时对模型精确程度的衡量。在两个模型 $m_1$ 和 $m_2$ 中，交叉熵越小的模型越准确（交叉熵永远不会低于真实熵，因此模型不会因为低估真实熵而犯错）。

当观察到的单词序列的长度趋于无穷大时，交叉熵被定义为极限。我们将需要一个近似的交叉熵，依赖于一个固定长度的（足够长的）序列。对单词序列 $W$ 的模型 $M=P(w_i|w_{i-N+1:i-1})$，这种近似的交叉熵为：
$$
H(W)=-\frac{1}{N}\log{P(w_1w_2...w_N)}\tag{2.2.13}
$$
一个模型 $P$ 在一个词序列 $W$ 上的困惑度现在被正式定义为2的交叉熵的幂：
$$
\begin{align}
Perplexity(W)&=2^{H(W)}\\
&=P(w_1w_2...w_N)^{-\frac{1}{N}}\\
&=\sqrt[N]{\frac{1}{P(w_1w_2...w_N)}}\\
&=\sqrt[N]{\prod_{i=1}^N\frac{1}{P(w_i|w_1...w_{i-1})}}
\end{align}\tag{2.2.14}
$$



> 交叉熵：其作用和相对熵一样，也是衡量两个分布之间的差异大小，其表达式如下：
> $$
> H(P,Q)=-\sum_{i=1}^NP(x_i)\log{Q(x_i)}\tag{2.2.15}
> $$
> 结合信息熵和相对熵的表达式，可得如下关系：
> $$
> H(P,Q)=D_{KL}(P,Q)+H(P)\tag{2.2.16}
> $$




#### 【3】 相对熵

**相对熵（KL散度）**：相对熵主要衡量**两个分布之间的差异**，描述了两个分布之间的**距离**。（不是距离函数求得的通常意义上的距离）
$$
\begin{align}
D_{KL}(P,Q)&= E_{x\sim p}[\log{\frac{P(x)}{Q(x)}}]\\
& = E_{x\sim p}[\log{P(x)}-\log{Q(x)}]\\
&= \sum_{i=1}^NP(x_i)[\log{P(x_i)}-\log{Q(x_i)}]\\
&= \sum_{i=1}^NP(x_i)\log{P(x_i)}-\sum_{i=1}^NP(x_i)\log{Q(x_i)}\\
&=H(P,Q)-H(P)
\end{align}\tag{2.2.17}
$$

- $P(x)$是真实的分布，$Q(x)$是模型预测出来的分布

---

**传统语言模型的限制：**

1. 如果采用n元语法，长距离依赖无法解决
2. 最大似然估计应用于n元语法，稀疏性无法解决
3. 泛化性差，black car, blue car的出现，并不能有助于我们预测red car



## 2.3 神经语言模型简介

非线性神经网络语言模型可以解决一些传统语言模型中的问题：增加上下文的同时参数仅呈线性增长，缓解了人工设计的需要，支持不同上下文泛化性能。

- 输入：k元语句（代码实现熵是 k 个词向量的拼接）
- 输出：下个词的概率分布P



我们的目标是希望神经网络发现如下的规律，于是有了网络结构图：
$$
P(C(w_i)|C(w_{i-4}),C(w_{i-3}),C(w_{i-2}),C(w_{i-1}))\tag{2.3.1}
$$

![image-20230618191118904](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306181911142.png)

<center style="color:blue">图2.3.1 网络结构图</center>

### 2.3.1 one-hot 表示

先用最简单的方式表示每个词，**one-hot 表示**为：

- $dog = (0, 0, 0, 0, 1, 0, 0, ...)$
- $cat = (0, 0, 0, 0 ,0 ,0, 0, 0 ,1, 0, ...)$
- $eat = (0, 1, 0, 0, ...)$

:one: 概述：

- 基于文本簇进行字典的构建，字典的长度为 $|V|$ 。
- 构建长度为 $|V|$ 的向量
- 在向量表示中，当前词在字典中的位置为1，其他位置都为0

![image-20230618191615505](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306181916576.png)

<center style="color:blue">图2.3.2 one-hot 表示</center>

:two: 缺点：

- 当语料库很大时，需要建立一个很大的字典，确保所有的单词进行索引编码
- 假设字典长度10万词，构建的向量的长度就是10万，太大了
- 基于 One-Hot 编码的向量是稀疏矩阵（只有一个1，其余都是0）
- 稀疏矩阵无法表达单词与单词之间的相似程度，因为稀疏矩阵是绝对正交的

 one-hot 表示法有诸多缺陷，还是稠密的向量表示更好一些，那么如何转换呢？只需要加一个隐藏层映射一下就好了。映射之后的向量层如果单独拿出来看，还有办法找到原来的词是什么吗？

![image-20230618191830200](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306181918260.png)

<center style="color:blue">图2.3.3 添加隐藏层映射</center>

one-hot 表示法这时候就作为一个索引字典了，可以通过映射矩阵对应到具体的词向量。

![image-20230618192010527](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306181920580.png)

<center style="color:blue">图2.3.4 映射矩阵</center>

这样，这个神经网络的每个环节都没问题了，可以开始训练了。

---

**独热编码**：让计算机认识单词

![image-20230617203749587](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306172038746.png)

<center style="color:blue">图2.3.5 独热编码</center>

词典 $V$：新华词典里面把所有的词集合成一个集合 $V$。

> 假设词典里面有 8 个单词，计算机不认识单词，但是我们要计算机认识单词。

独热编码：给出一个 $8 \times 8$ 的矩阵。

- "time"：10000000
- "fruit"：01000000
- ……
- "banana"：00000001

余弦相似度 去计算两者的相似度，发现独热编码 计算得到的余弦相似度都为0，即这些单词都没有关联度（独热编码缺陷），于是就有了**词向量**的概念。



**数学公式总结**：


输入向量 $x$，输出结果 $y$ 的概率分布，两者的表达式如下所示，其中 $w$ 代表词向量，$v$ 代表词到词嵌入的映射，$LM$ （Lamguage Model）是一个多层感知机：
$$
\begin{align}{100}
LM(w_{1:k})&=softmax(hW^2+b^2)\\
&x = [v(w_1);v(w_2);...;v(w_k)]\\
&h=g(xW^1+b^1)
\end{align}\tag{2.3.2}
$$

- $v$ 就是映射矩阵
- $w$ 是输入的词向量
- $x$ 是将前 k 个词通过映射矩阵处理后的一个拼接
- $h$ 是隐藏层
- $W^1$ 和 $W^2$ 表示两个矩阵



### 2.3.2 神经网络语言模型（NNLM）

![image-20230618193614935](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306181936991.png)

<center style="color:blue">图2.3.5 NNLM_1</center>



![image-20230617204634468](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306172046542.png)

<center style="color:blue">图2.3.6 NNLM_2</center>

假设有4个单词：$w_1,w_2,w_3,w_4$（4个单词的独热编码）。

1. $w_1 \times Q = c_1,\space \space w_2 \times Q = c_2,\space\space w_3 \times Q = c_3,\space\space w_4 \times Q = c_4$。
   - $Q$ 就是一个随机矩阵（可学习），是一个参数
2. $C = [c_1,c_2,c_3,c_4]$。
3. $softmax(U[tanh(WC+b_1)]+b_2) == [ 0.1,\space0.2,\space0.5,\space0.2 ]\in[1,V_L]$。最后生成一个一维矩阵，其中矩阵的长度为词典的长度 $V_L$



### 2.3.3 词向量（神经网络语言模型的副产品 $Q$）

给我任何一个词（“判断”），会给出相应的独热编码 $[0,0,1,0,...,0]$。

- $W_1 \times Q = c_1$，$c_1$ 就是 “判断” 这个词的词向量。

**词向量**：就是用一个向量来表示一个单词。独热编码也属于词向量，只是独热编码存储空间太大，且两者的余弦相似度都为0。

经过 $Q$ 处理，可以控制词向量的维度（大小），也解决了相似度的问题。

> 通过神经网络语言模型，找到一个合适的 $Q$ 矩阵，得到一个合适的词向量，这个词向量能够更加准确的表示这个词



### 2.3.4 案例实现

```py
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from torch.autograd import Variable

dtype = torch.FloatTensor

sentences = ["i like dog", "i love coffee", "i hate milk", "i do nlp"]

word_list = ' '.join(sentences).split()
word_list = list(set(word_list))

word_dict = {w: i for i, w in enumerate(word_list)}
number_dict = {i: w for i, w in enumerate(word_list)}
# print(word_dict)

n_class = len(word_dict)

m = 2
n_step = 2
n_hidden = 2


def make_batch(sentence):
    input_batch = []
    target_batch = []

    for sen in sentence:
        word = sen.split()
        input = [word_dict[n] for n in word[:-1]]
        target = word_dict[word[-1]]

        input_batch.append(input)
        target_batch.append(target)

    return input_batch, target_batch


class NNLM(nn.Module):
    def __init__(self):
        super(NNLM, self).__init__()
        self.embed = nn.Embedding(n_class, m)
        self.W = nn.Parameter(torch.randn(n_step * m, n_hidden).type(dtype))
        self.d = nn.Parameter(torch.randn(n_hidden).type(dtype))
        self.U = nn.Parameter(torch.randn(n_hidden, n_class).type(dtype))
        self.b = nn.Parameter(torch.randn(n_class).type(dtype))

    def forward(self, x):
        x = self.embed(x)  # 4 x 2 x 2
        x = x.view(-1, n_step * m)
        tanh = torch.tanh(self.d + torch.mm(x, self.W))  # 4 x 2
        output = self.b + torch.mm(tanh, self.U)
        return output


model = NNLM()

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

input_batch, target_batch = make_batch(sentences)
input_batch = Variable(torch.LongTensor(input_batch))
target_batch = Variable(torch.LongTensor(target_batch))

for epoch in range(5000):
    optimizer.zero_grad()

    output = model(input_batch)  # input: 4 x 2

    loss = criterion(output, target_batch)

    if (epoch + 1) % 1000 == 0:
        print('epoch:', '%04d' % (epoch + 1), 'cost = {:.6f}'.format(loss.item()))

    loss.backward()
    optimizer.step()

predict = model(input_batch).data.max(1, keepdim=True)[1]

print([sen.split()[:2] for sen in sentences], '->', [number_dict[n.item()] for n in predict.squeeze()])
```

![image-20230623162348564](https://bucket-zly.oss-cn-beijing.aliyuncs.com/img/Typora/202306231623675.png)



## 2.4 预训练的词表示（word embedding）

- 介绍：通常是一个LM的副产物
- 种类举例：Word2vec、GloVe、FastText、ELMo、GPT、BERT